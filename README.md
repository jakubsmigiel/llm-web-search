# Web Research LLM AI Agent

I wanted to experiment with automating complex web research with Large Language Models. From my experience, LLMs are ~~stupid~~ not reliable for decision making but they are a great tool for text processing. For this reason, I decided to not let the agent decide for itself and I developed a rigid framework to guide it, while levaraging the strengths of the different models I employed.

## The framework

### Simple
1. Starting with a question from a user, generate a Google search query with `gemma3n:e4b`.
2. For each of the results:
   1. Scrape the website,
   2. Summarize all the information relevant to the question with `gemma3n:e4b`,
   3. Ask `gemma3n:e4b` whether the summary contains relevant information answering the question and
      1. if not, then skip this result and don't use the summary,
      2. if yes, then store the summary of the search result for later,
   4. Continue until a total of 6 results has been collected.
3. Use `gemma3:27b` to collate the summaries into a final report, including references to sources after each point and section.

### Extended

The extended framework starts with the same process as the simple framework, then contines working on the report:
1. Use `gemma3:27b` to write criticism of the first report, including potential areas for further research where information is missing.
2. Let `gemma3:27b` pick which subjects should be explored next to extend the report.
3. Run the simple framework on each of the follow-up subjects.
4. Collate the follow-up reports with the original report using `gemma3:27b`, one by one, until a final full report is reached.

### Caching
Because the process can take a long time, I implemented caching of the state at distinct steps, so that research can be paused and resumed if the same question is typed in.

## Examples
### Simple framework
- [How does a blind person read PDF documents?](examples/blind-pdf.md)
- [What is the distance from the Earth to the Sun?](examples/earth-sun.md)

### Extended framework
- [How to make perfect espresso?](examples/espresso.md)

## Model choices
In my experiments, I learned the strengths and weaknesses of many models. The justification for settling with the two is as follows.
- `gemma3n:e4b` - a fast model, good for writing summaries and answering questions, however it doesn't deal well with tasks on long text and sometimes doesn't follow instructions. Good for extracting information from sources.
- `gemma3:27b` - works very slowly but follows instructions very well and can deal with long text, providing reliable results. Good for collecting the summaries into one text in the formatting specified in the prompt.

## Usage and installation

To install required packages, run
```
pip3 install trafilatura ollama googlesearch-python
```

To run the script, execute `main.py`
```
$ python3 main.py 
question: what is the distance from the earth to the sun?
extend the report when it's done? [y/n] n
```

The LLMs are hosted locally with [ollama](https://ollama.com/). After installing it on your computer, start serving the service and install the models
```
ollama serve
ollama pull gemma3n:e4b gemma3:27b
```


## Ethical considerations and warranty
This software comes with no warranty of any kind, including that the information collected and generated by this agent is true and properly fact-checked. While the results produced by this software look very convincing, the sources match, and were fact-checked on a couple questions by people experienced in the relative topics, LLMs are known to hallucinate false information and behave randomly. This software has its use-cases and can be very helpful, especially as a starter for own research, but the information it generates should be treated skeptically.

The simple framework takes about 1 hour to complete the research and the extended framework can take a multiple of that proportional to the number of followup questions, usually about 4 hours. The models run on a PC, so the power consumption of this agent should be taken into account.

If the agent was built to run on cloud AI providers' services, the LLMs would write faster and a lot of the work could be done in parallel, however I decided to run it locally for privacy and other moral reasons.